---
title: "Tp Statistiques 2"
author: "Lapu Matthias | Amaël Kreis"
output: 
  pdf_document:
    latex_engine : xelatex
---

# Simulation et convergence

## I. Variation sous-jacente et échantillonnage répété

1.  Si X ∼ E(0.5), quelle est la probabilité qu'on observe une valeur supérieure à 3?

On a $$ f(0.5) = 0.5e^{-0.5x} $$
Donc pour $$ X\sim\xi(0.5) $$
On trouve
$$
P(X \gt 3)=
\int_{3}^{+\infty} \frac{1}{2}e^{\frac{-x}{2}}dx
= \left[-e^{-\frac{x}{2}}\right]_3^{+\infty}
=e^{-\frac{3}{2}}\approx0.223
$$

2.Simulez un échantillon de taille n = 20 d'un loi de E(0,5), créez un histogramme de votre échantillon et commentez la forme de votre histogramme. Superposer la vrai densité. Quelle est la probabilité empirique qu'on observe une valeur supérieure à 3 ?

```{r q2}
x<-rexp(20,0.5)
hist(x, freq=FALSE)
maxvalue <- ceiling(max(x))
lines(0:maxvalue,dexp(0:maxvalue, 0.5), col="green",)
```

Commentaire histogramme à insérer

3.Répétez cette opération 5 ou 6 fois et commentez les différences entre les histogrammes que vous obtenez à chaque fois. Utilisez la même limite sur les axes pour faciliter la comparaison. Notez également comment la probabilité empirique qu'on observe une valeur supérieure à 3 change.

```{r}

```

4.  Augmentez la taille de votre échantillon à 100 et répétez votre expérience. Que remarquez-vous?

```{r, echo=FALSE}
x<-rexp(100,0.5)
hist(x, freq=FALSE)
maxvalue <- ceiling(max(x));
lines(0:maxvalue,dexp(0:maxvalue, 0.5), col="blue",)
```

## II. Variabilité aléatoire du maximum de l'échantillon

1.  Simuler un échantillon de taille n = 10 d'une loi U(−1, 1) et enregistrez le maximum de l'échantillon.

```{r}
x <- runif(10, -1, 1)
max <- max(x)
```

2.  Répétez les deux étapes ci-dessus dix fois, en écrivant le maximum de l'échantillon à chaque fois. Commentez la variabilité des valeurs que vous obtenez pour les maxima de votre échantillon.

```{r}
for (i in 1:10) {
  x <- runif(10, -1, 1)
}
```

3.  Répétez 100 fois et construisez un histogramme et une boîte à moustaches. Quelle est la loi dumaximum, M = max 1≤i≤n X i où X i ∼ U(−1, 1) (TD1) ? Superposer la densité théoreique sur l'histogramme. Que remarquez-vous ?

```{r}
par(mfrow=c(3,4))
for (i in 1:100) {
  x <- runif(10, -1, 1)
  hist(x)
  boxplot(x, horizontal = TRUE)
}
```

4.  Augmentez la taille de votre échantillon à 50 et répétez votre expérience. Que remarquez-vous? Sont-ils proches de la symétrie ?

```{r}
x <- runif(50, -1, 1)
```

# Monte Carlo Methods

## Moyenne et phénomène de concentration.

1.  Donner une borne de cette quantité en utilisant l'inégalité de Bienaymé Chebychev.

Inégalité de Bienaymé Chebychev $$
P(| \hat{\theta}-\theta| \ge \delta) \le \frac{V(\hat{\theta})}{\delta²}
$$

Calculons la variance grâce à son caractère quadratique ainsi qu'à l'indépendance des X_i : $$
V(\hat{\theta}) = V(\frac{1}{n} \sum_{i=1}^{n} \psi(X)) = \frac{1}{n²}V(\sum_{i=1}^n \psi(X)) = \frac{1}{n²} \sum_{i=1}^n V(\psi(X)) = \frac{1}{n²} *n\sigma² = \frac{\sigma²}{n}
$$ On retrouve donc une borne pour cette inégalité.

2\. En supposant que a ≤ ψ(Xi) ≤ b, donner une borne en utilisant l'inégalité de Hoeffding.

Posons : $$
S_{n} = \sum_{k=1}^n \psi(X_k)
$$ D'après l'énoncé, nous savons que : $$
\frac{1}{n} \sum_{i=1}^n \psi(X_i) = \hat{\theta}
$$ Ainsi : $$
\frac{S_n}{n} = \hat{\theta} 
$$ Donc : $$
S_n=n\hat{\theta}
$$

D'après l'inégalité de Hoeffding nous savons que : $$
P(|S_n - E(S_n)| \geq t) \leq 2exp(\frac{-2t²}{\sum_{k=1}^n (b_k-a_k)²})
$$

Calculons l'esperance de S_n: $$
E(S_n) = E(\sum_{k=1}^n \psi(X_k)) = \sum_{k=1}^n E(\psi(X_k)) = \sum_{k=1}^n \theta = n\theta
$$ Ainsi : $$
P(|n\hat{\theta} - n\theta| \geq t) \leq 2exp(\frac{-2t²}{\sum_{k=1}^n (b_k-a_k)²})
$$ $$
P(|\hat{\theta} - \theta| \geq \frac{t}{n}) \leq 2exp(\frac{-2t²}{\sum_{k=1}^n (b_k-a_k)²})
$$ On pose : $\frac{t}{n} = \delta$

$$
P(|\hat{\theta} - \theta| \geq \delta) \leq 2exp(\frac{-(2n\delta)²}{\sum_{k=1}^n (b_k-a_k)²})
$$ 3. De combien d'échantillons auriez-vous besoin pour que la probabilité que \delta = 2\phi soit inférieur à 1%.

Servons-nous de l'inégalité de Bienaymé-Tchebychev que nous avons trouvé lors du 1.

$$
P(|\hat{\theta} - \theta| \ge \delta) \le \frac{\delta²}{n\sigma²}
$$ Remplaçons par : $$\delta = 2\sigma$$ $$\delta²=4\sigma²$$ Ainsi : $$
\frac{\sigma²}{n\delta²} = \frac{1}{4n}
$$ Or l'énoncé demande à ce que la probabilité soit inférieur à 1% , c'est-à-dire que : $$
\frac{1}{4n} = 0.01  \Longrightarrow n=25
$$ Afin que la probabilité soit inférieur à 1%, il faut 25 échantillons.

## Application pour l'estimation de probabilité

1\. Pour la question 1 :

Le paramètre d'intérêt est la moyenne de la fonction exponentielle, c'est à dire 2

Un estimateur serait : $$
E(\epsilon(\theta)) = \frac{1}{\theta}
\bar{X_n} = \frac{1}{n} \sum_{i=1}^{n} X_i
\hat{\theta_n} = \frac{1}{\bar{X_n}}
$$ La moyenne empirique tend presque surement vers la moyenne , ainsi l'estimateur est donc consistant. Il faut ensuite utiliser l'inégalité de Hoeffding.

L'énoncé demande à ce que E(Z) = garantie probabiliste de l'erreur. Il faut trouver Z .

On pose Z = $$
Z = \psi(| \hat{\theta} - \theta |)
$$ En calculant l'esperance nous avons donc : $$
E(Z)= \int \psi(|\hat{\theta} - \theta|)f(x)dx
$$

## Théorème Central Limite et Estimation Monte Carlo

1\. Vérifier que l'espérance théorique d'une loi de Pareto est E [X] = αa/α−1.

$$
P(X\le t)= (1-\left( \frac{a}{t} \right)^{\alpha}) , \;avec \;x \ge a
$$

Donc : $$
E(X) = \int_0^{+\infty} 1-P(X \le t)dt =
\int_0^{+\infty} P(X    > t) dt =
a+a^{\alpha}\int_a^{+\infty}\frac{1}{t^{\alpha}}dt=a + \frac{a}{\alpha -1} =
\frac{\alpha a}{\alpha -1}
$$

2\. Simuler N = 1000 échantillons i.i.d de loi commune Pareto P(a, α) (avec votre choix de paramètres)

de taille n = 5, 30, 100 et calculer les moyennes et variances empiriques ¯Xn,i et Sn,i, i = 1, . . . ,N.

```{r echantillons,echo=FALSE}
s<-library("EnvStats")
#vars
N <- 1000
a <- 1
alpha <- 3
#creation des echantillons
ech5 <- matrix(NA,5,1000) #echantillon de taille 5
for (i in seq(5)) {
  ech5[i,] <- rpareto(N,a,alpha)
}
ech30 <- matrix(NA,30,1000) #echantillon de taille 30
for (i in seq(30)) {
  ech30[i,] <- rpareto(N,a,alpha)
}
ech100 <- matrix(NA,100,1000)  #echantillon de taille 100
for (i in seq(100)) {
  ech100[i,] <- rpareto(N,a,alpha)
}
print("Moyenne empirique n = 5")
moy5 <- rowMeans(ech5)
print(moy5)
print("Moyenne empirique n = 30")
moy30 <- rowMeans(ech30)
print(moy30)
print("Moyenne empirique n = 100")
moy100 <- rowMeans(ech100)
print(moy100)
print("Variance empirique n = 5")
print(moy5^2/1000)
print("Variance empirique n = 30")
print(moy30^2/1000)
print("Variance empirique n = 100")
print(moy100^2/1000)
hist(moy5)
hist(moy30,breaks = 10)
hist(moy100,breaks = 20)
```

4.  A l'aide d'une renormalisation adéquate (an, bn), montrer que Un,i = ¯Xn,i−an/ bn a une loi que vous pouvez approchez. Comparez histogramme de les moyennes empiriques normalisées, Un,i, et distribution théorique approchée. Quelle est l'influence de la taille de l'échantillon n sur la qualité de cette approximation?

```{r normalisation, }
moy5CentreeReduite <- (moy5-mean(moy5))/mean(moy5^2/1000)
moy30CentreeReduite <- (moy30-mean(moy30))/mean(moy30^2/1000)
moy100CentreeReduite <- (moy100-mean(moy100))/mean(moy100^2/1000)

hist(moy5CentreeReduite,freq=FALSE)
lines(seq(-50,50,by=0.1),dpareto(seq(-50,50,by=0.1),1,3),col = "blue")
hist(moy30CentreeReduite,freq=FALSE,breaks = 10)
lines(seq(-50,50,by=0.1),dpareto(seq(-50,50,by=0.1),1,3),col = "blue")
hist(moy100CentreeReduite,freq=FALSE,breaks = 20)
lines(seq(-50,50,by=0.1),dpareto(seq(-50,50,by=0.1),1,3),col = "blue")
```

## Quand le théorème de central limite ne s'applique pas

1\. Simuler un échantillon de taille n = 20 d'une loi de C(2) et calculer la moyenne empirique ¯Xn.

Moyenne empirique:

```{r, echo=FALSE}
cauchy20 <- rcauchy(20, scale=2)
print(mean(cauchy20))
```

2\. Faites varier la taille de l'échantillon n = 20, 100, 1000 et 10000. Qu'en déduire ?

```{r,echo=FALSE}
cauchy100 <- rcauchy(100,scale=2)
cauchy1000 <- rcauchy(1000,scale=2)
cauchy10000 <- rcauchy(10000,scale=2)
print(mean(cauchy20))
print(mean(cauchy100))
print(mean(cauchy1000))
print(mean(cauchy10000))
```

On remarque que malgré le nombre élévé de l'échantillon la moyenne ne semble pas se stabiliser comme pour une loi normale.

3\. Expliquer ce comportement

Nous savons d'après le cours de probabilités que la loi de cauchy n'admet pas d'espérance ni d'écart type. Cela explique donc le comportement de la moyenne malgré la taille de l'échantillon.

4\. Quelle est la médiane d'une loi de cauchy ?

La courbe est symétrique ,la médiane d'une loi de cauchy est \theta. D'après RStudio, quand la position n'est pas défini celle-ci est mise entre à 0. Par conséquent nous devons vérifier si la médiane semble proche de 0.

$$
f(x,\theta) = \frac{1}{\pi} \frac{1}{1 + (x-\theta)²}
\frac{1}{2} = \frac{1}{\pi} \int_{-a}^{a} \frac{dx}{1+(x-\theta)²}
F^{-1} (\frac{1}{2}) = \theta
$$

5\. En déduire un estimateur de theta et evaluer la performance de cet estimateur sur les différents échantillons.

Nous pouvons essayer d'approximer theta , c'est-à-dire la médiane, cela revient donc à chercher une estimation du quantile en 0.5 . D'après le cours, les quantiles permettent de localiser les valeurs les plus fréquentes. Nous allons donc essayer d'estimer le quantile.

```{r}
hist(cauchy20,breaks=20)
abline(v=mean(cauchy20),col="red",lwd=3)
hist(cauchy100,breaks=100)
abline(v=mean(cauchy100),col="red",lwd=3)
hist(cauchy1000,xlim=c(-100,100),breaks=1000)
abline(v=mean(cauchy1000),col="red",lwd=3)
hist(cauchy10000,xlim=c(-100,100),breaks=10000)
abline(v=mean(cauchy10000),col="red",lwd=3)
```

D'après les graphiques nous pouvons remarquer que l'estimation de theta semble proche de la vrai valeur, n'ayant pas de moyen de calculer l'espérance de la loi cela semble être un bon estimateur, car celui-ci semble proche de 0, assez pour jugé les performances de cet estimateur comme suffisant.
